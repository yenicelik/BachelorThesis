%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Extensions}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

\section{Finding a matrix when we apply a polynomial kernel onto the input first}
We want to analyse if the gpregression can identify the random matrix, if the input is first put through a polynomial kernel of degree 2. \\

More specifically, the problem looks as follows.
We want to approximate the real function $ f $ through a gaussian process $ g $, with the following condition:

\def\B{
\begin{bmatrix}
    (x - x_0)^2 \\
    (x - x_1)^2
\end{bmatrix}}

\begin{equation}
f \left( W \B \right) \approx g \left( x \right)
\end{equation} 

where $x_0, x_1$ are constants.

\section{Our proposed improvement to existing methods}
We propose a novel method which is based on additive GPs, where we use different kinds of kernels.

We model the function $f$, which we approximate through functions $g_{i}$ as follows:

\begin{equation}
f(x) \sim g_1(A_1 x_1) + \sum_{i} g_i(A_i x_i)
\end{equation}

where $x_1$ is the subvector of the input $x$ that includes the active subspace.
And each $x_i$ is a subvector of the input $x$ which is part of the nullspace.

We propose the following training method:

\begin{enumerate}
\item We first approximate the function 
$$ f(x) \sim g_1(A_1, x) $$.
We decide the dimensionality of the matrix $A_1$ by deciding a 'cutoff dimension' as proposed by Tripathy et al. in their Algorithm 4.
$g_1$ is a gaussian process, and $A_1$ is a learned matrix.

\item We then expand this approximated term to
$$ f(x) \sim g_1(A_1, x)  + g_2(A_2, x)$$.
In this step, $g_1$ and $A_1$ are kept fix.
We now learn the function $g_2$ using a gaussian process, and $A_2$ using a matrix optimization process.
We must make sure that $A_1$ and $A_2$ do not map onto the same subspace!

\item We repeat the above procedure until we have the following expression:
$$f(x) \sim g_1(A_1 x_1) + \sum_{i} g_i(A_i x_i)$$, where we refer to $g_1$ as the leading term, and $g_{i=2,...i_{max}}$ as the follow-up terms.


\end{enumerate}






