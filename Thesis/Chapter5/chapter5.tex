%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Fourth Chapter **********************************
%*******************************************************************************
\chapter{Model Design and Extensions to the state of the art}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

Given the fields of improvements in the above section, we now propose an algorithm which addresses the majority of the issues mentioned in the above section.
We will first present that algorithm, and then points out, as to why each individual concern is addressed.

\section{The BORING Algorithm}

We propose the following algorithm, called BORING. \textbf{BORING} stands for \textbf{B}ayesian \textbf{O}ptimization using \textbf{R}EMBO and \textbf{I}de\textbf{N}tifyable subspace \textbf{G}eneration.

The general idea of boring can be captured in one formula, where $f$ stands for the real function that we wish to approximate, and any subsequent function annotated by $g$ refers to a component of the right hand side, which approximates the function $f$.

\begin{equation}
f(x) \approx g_0(A x) + \sum_{i \in \mathbb{Z}^+}^{q} g_i( A^{\bot} x)_i )
\label{eq:dimRedEquation}
\end{equation} \\

Where the following variables have the following meaning
\begin{itemize}
\item $A$ is the active subspace projection (an element of the stiefel manifold) learned through our algorithm, using Algorithm 1
\item $A^{\bot}$ is an matrix whose subspace is orthonormal to the projection of $A$.
We generate $A^{\bot}$ using Algorithm 2.
\end{itemize}

We will no proceed with describing the algorithm in more detail.

\subsection{Algorithm Description}

\begin{algorithm}
\caption{BORING Alg. 1}

\begin{algorithmic} 
\STATE $X \leftarrow \emptyset$
\STATE $Y \leftarrow \emptyset$

\WHILE{budget not depleted}
\STATE $ q(R) \leftarrow \text{LAPLACEAPPROX}( p(R | X, Y, \kappa, \mu) ) $
\STATE $ q(f) \leftarrow  APPROXMARGINAL( p(f | R), q(R)) $
\STATE $ x_* \leftarrow OPTIMIZEUTILITY( q(f), q(R) )$
\STATE $ y \leftarrow OBSERVE( f( x_* ) ) $
\STATE $ X \leftarrow [X; x_*] $
\STATE $ Y \leftarrow[Y; y_*] $
\ENDWHILE

\RETURN $q(R), q(f)$
\end{algorithmic}

\end{algorithm}

We find the active projection matrix using the following algorithm, which is identical to the procedure described in "CITE TRIPATHY".

\begin{algorithm}
\caption{BORING Alg. 2} 

\begin{algorithmic} 
\STATE $X \leftarrow \emptyset$
\STATE $Y \leftarrow \emptyset$

\WHILE{budget not depleted}
\STATE $ q(R) \leftarrow \text{LAPLACEAPPROX}( p(R | X, Y, \kappa, \mu) ) $
\STATE $ q(f) \leftarrow  APPROXMARGINAL( p(f | R), q(R)) $
\STATE $ x_* \leftarrow OPTIMIZEUTILITY( q(f), q(R) )$
\STATE $ y \leftarrow OBSERVE( f( x_* ) ) $
\STATE $ X \leftarrow [X; x_*] $
\STATE $ Y \leftarrow[Y; y_*] $
\ENDWHILE

\RETURN $q(R), q(f)$
\end{algorithmic}
\end{algorithm}

We then generate a matrix $ A^{\bot} $ by using the following procedure.

\begin{algorithm}[H]
\caption{BORING Alg. 3 - generate orthogonal matrix to A(A, n) }

\begin{algorithmic} 
\REQUIRE $A$ a matrix to which we want to create $A^{\bot}$ for; $n$, the number of vectors in $A^{\bot}$.

\STATE normedA $ \leftarrow $ normalize each column of $A$
\STATE $Q \leftarrow$ emptyMatrix()
\COMMENT{ The final concatenated $Q$ will be $A^{\bot}$. }
\FOR{i = 1,...,n}
\STATE $i \leftarrow 0$ 
\WHILE{i < 50}
\STATE i++
\STATE $q_i \leftarrow $ random vector with norm 1
\STATE newBasis = apply gram schmidt single vector( $[A, Q], q_i$ ) 

\IF{ dot(normed$A^T$, newBasis) $\approx \mathbf{0}$ and $|$ newBasis $|$ $> 1e-6$}
\STATE $Q \leftarrow$ colwiseConcatenate( $(Q, $ newBasis)
\STATE break
\ENDIF
\ENDWHILE                
\ENDFOR

\RETURN $Q$
\end{algorithmic}
\end{algorithm}



\subsection{Algorithm Properties}


\begin{equation}
A = 
\begin{bmatrix}
 \vdots & \vdots & & \vdots \\
 a_1 & a_2 & ... & a_{d_e} \\
 \vdots & \vdots & & \vdots
\end{bmatrix}
\label{eq:maximalEmbedding}
\end{equation}

Given that we choose a maximal lower dimensional embedding (maximising the log-likelihood of the embedding for the given points), some other axes may be disregarded.
However, the axes that are disregarded may still carry information that can make search faster or more robust.

To enable a trade-off between time and searchspace, we propose the following mechanism.

%% TODO I think it's the rowspace.. not the columnspace!
%% Potentially use this to generate these vectors: https://stackoverflow.com/questions/33658620/generating-two-orthogonal-vectors-that-are-orthogonal-to-a-particular-direction

Assume we have found the maximal embedding \ref{eq:maximalEmbedding}.
Then we have found the active subspace, which is characterizable through it's first few column vector $  a_1, a_2, ..., a_{d_e} $.
However, as said before, we also want to address the subspace which is not addressed by the maximal embedding, which we will refer to \textit{passive subspace}.
This passive subspace can be characterized by the set of vectors, that are all orthogonal to all other column vectors in $A$, i.e. the space orthogonal to the projection of $A$.

As such, we define the entire span of the given vectorspace as:

\begin{equation}
S = 
\begin{bmatrix}
A & V
\end{bmatrix}
\label{eq:entireSubspace}
\end{equation}

where $V$ describes the matrix that is orthogonal to the columnspace of $A$.
For this, $V$ consists of any basis vectors that are orthogonal to all other vectors in $A$.\\

We can generate these vectors by taking a random vector, and applying gram schmidt.
We repeat the above procedure if the norm of the resulting vector is smaller than a given threshold.
 (One could also maybe use fourier, then add another fourier axis, and then go back again to have a nice basis?). \\
 
 Now to the heart of the algorithm. 
 We want to calculate $g_i$ and $f$ within that \ref{eq:dimRedEquation}, such that the maximum likelihood of the data we have accumulated so far.
 Because the term can be written as a sum of expressions, we can maximize each summand individually, which will lead to maximizing the entire expression (after we have calculate the active subspace). \\
 
 The following few steps are applied after a "burn-in" phase, in which we use REMBO to acquire new points (as this is an efficient form of random sampling).
 We take $ \sqrt{d} $ as the dimensionality of the space $Y$ in which we search (heuristic!!).
 
 \begin{enumerate}
 \item Calculate the active subspace using the algorithm from 
 \item Calculate an appropriate pair of vectors $v_1, \ldots, v_{n-{q}}$, where each vector is orthonormal to every other vector in $A$.
 \item Maximize the GP for each individual expression of the space orthogonal to $A$ (as given by $V x$) individually.
 \end{enumerate}
 
 This fights the curse of dimensionality, as we can freely choose $q \geq d_e$ to set the complexity of the second term.
 This, thus allows for smaller perturbations in the space orthogonal to $A$ to occur.

\section{Additive Stiefel projections - Our proposed improvement to existing methods}
We propose a novel method which is based on additive GPs, where we use different kinds of kernels.

We model the function $f$, which we approximate through functions $g_{i}$ as follows:

\begin{equation}
f(x) \sim g_1(A_1 x_1) + \sum_{i} g_i(A_i x_i)
\end{equation}

where $x_1$ is the subvector of the input $x$ that includes the active subspace.
And each $x_i$ is a subvector of the input $x$ which is part of the nullspace.

We propose the following training method:

\begin{enumerate}
\item We first approximate the function 
$$ f(x) \sim g_1(A_1, x) $$.
We decide the dimensionality of the matrix $A_1$ by deciding a 'cutoff dimension' as proposed by Tripathy et al. in their Algorithm 4.
$g_1$ is a gaussian process, and $A_1$ is a learned matrix.

\item We then expand this approximated term to
$$ f(x) \sim g_1(A_1, x)  + g_2(A_2, x)$$.
In this step, $g_1$ and $A_1$ are kept fix.
We now learn the function $g_2$ using a gaussian process, and $A_2$ using a matrix optimization process.
We must make sure that $A_1$ and $A_2$ do not map onto the same subspace!

\item We repeat the above procedure until we have the following expression:
$$f(x) \sim g_1(A_1 x_1) + \sum_{i} g_i(A_i x_i)$$, where we refer to $g_1$ as the leading term, and $g_{i=2,...i_{max}}$ as the follow-up terms.

\end{enumerate}

The question now is: Does this structure implicitly find additive substructures between variables, and how is this better than actively finding substructures?

\subsection{What is it better than actively finding substructures}.
Actively finding substructures does find relations between different variables.
But it does not account for the weight each variable has within the subsubstruce, which then needs to be accounted by.

We will refer to a smaller function that does not influence the real function $f$ by $O(g_n)$.

We are now interested if it covers some edge cases.

We separate the algorithm into two parts:

1.) Is the model capable of learning this structure
2.) How could training result in this structure, and are the chances big enough?


\section{Random addition of REMBO}
Another model we potentially propose is again based on additive GP's, where we use again a mixture of different kernels.

We model the function by the following term:

% f(x) \sim g_1(A_1 x_1) +g_1(A_1 x_1) + g_1(A_1 x_1) + g_1(A_1 x_1) + \sum_{i} g_i(A_i x_i) + \ldots

\begin{equation}
f(x) \sim \sum_i g_i^1(A_i^1 x) + \sum_i g_i^2(A_i^2 x) + \sum_i g_i^3(A_i^3 x) + \ldots + \sum_i g_i^{d_{max}}(A_i^{d_{max}} x)
\end{equation}

The idea is that this model can capture the majority of all the possible substructures of a matrix up to a certain dimension $d_{max}$.
We can randomly take and leave behind certain terms from within the above expression.
We define this number to be $\epsilon$, where we leave out one term with probability $\epsilon$.
We can then train the above substructure easily, as the random projection matrices may be fixed.

----
Downsides - this is basically "a random version to find additive substructures within the subspace X".

In minization, we can minize both to best fit the predicted mean (on a test set), and the predicted variance over all values.

--> Possible extension to the Projection pursuit regression?

\section{Additive variance reducing GP}
For this technique, we must detect the number of possible cliques.
We don't need to know which variables go together, however, rather, how many times we get a maximal clique.

We can use the EPFL junction tree thing to find how many maximal cliques we have. 
Then we spawn that many terms with random matrices, but first project them all onto a lower subspace.
How do we find out how many active dimensions each of these values has?
Or do we assume that each such clique already is in a lower dimension.


Besides fitting a set of test data well through the predicted mean function, we also wish to minimize the 

\subsubsection{Why this algorithm?}

\begin{enumerate}
\item Our algorithm intrinsically uses multiple restarts, firstly proposed as an extension to REMBO.
This makes training more reliable.
\item Our algorithm allows to not only optimize on a given domain, but also identify the subspace on which the maximal embedding is allocated on.
\item Our algorithms uses a "burn-in-rate" for the first few samples, which allows for efficient point search at the beginning, and later on switches to finding the actual, real subspace.
\item Our algorithm has faster convergence onto the real subspace, as we use the most promising models during optimization, instead of going through all possible restarts. (Approximate enhancement of loss by taking sum of last 10 losses - extrapolate, and compare it to the best example so far!)
\item Our algorithm is more accurate, as we don't assume that there is a singular maximal subspace. 
We also take into consideration that there might be perturbation on lower dimensions!
\end{enumerate}



%% \begin{equation}
%%
%% \end{equation}




\section{Why ours}
REMBO has a probaility of failing. This probability is not high, but may result in very, very bad results. 
Our model gives an approximate best solution given the data, which means that it always performs better than REMBO (because of the built-in restarts).


