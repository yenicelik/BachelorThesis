%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Conclusion}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter7/Figs/Raster/}{Chapter7/Figs/PDF/}{Chapter7/Figs/}}
\else
    \graphicspath{{Chapter7/Figs/Vector/}{Chapter7/Figs/}}
\fi

\section{Main Contributions}
I present this thesis as a result of a systematic treatment of the topic "Bayesian Optimization in High Dimensions."

\begin{enumerate}
\item BORING allows the number of dimensions to be smaller than the real active subspace.
Although in general, it does not perform better than other subspace identification algorithms (like Tripathy's method), it does allow for better values when the subspace identification fails, as it incorporates an additional dimension which acts as a random embedding.
As such, it allows for a good trade-off between subspace identification, and optimization using random projections.
\item BORING is more robust to smaller perturbations, as it does not rely on finding the exact active subspace.
It allows optimizing over smaller perturbations as well, whereas current algorithms usually don't take these into consideration.
\item I analyze the relationship between improved log-likelihood and angle difference. 
I see that it is not surprising that the angle difference between the real projection matrix and the found projection matrix is not tightly coupled to the log-likelihood.
However, improving over the log-likelihood can reduce the angle-difference in many cases.
\end{enumerate}

\section{Future work}

Future work could incorporate the synthesis of different methods, including additive GPs and Tripathy's method.
Although Tripathy's method is unbeaten in identifying the active subspace dimension, heuristics could be easily implemented to speed up the calculation time.
One of the most critical aspects is hyperparameter tuning for the GP models itself.
These can make or break the identification of the subspace.
Choosing bad hyperparameters does not allow us to compare different methods effectively.
In the future, it would be beneficial to address this issue to a stronger extent.
Recalculating the search space for each new point may be too time-consuming, with which REMBO would still be considered state of the art regarding Bayesian Black Box Optimization.
 



