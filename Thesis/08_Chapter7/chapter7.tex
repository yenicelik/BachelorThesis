%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Conclusion}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter7/Figs/Raster/}{Chapter7/Figs/PDF/}{Chapter7/Figs/}}
\else
    \graphicspath{{Chapter7/Figs/Vector/}{Chapter7/Figs/}}
\fi

\section{Main Contributions}
I present this thesis as a result of a systematic treatment of the topic "Bayesian Optimization in High Dimensions".

\begin{enumerate}
\item BORING allows the number of dimensions to be smaller than the real active subspace.
Although in general, it does not perform better than other subspace identification algorithms (like tripathy's method), it does allow for better values when the subspace identification fails, as it incorporates an additional dimension which acts like a random embedding.
As such, it allows for a good trade-off between subspace identification, and optimization using random projections.
\item BORING is more robust to smaller perturbations, as it does not rely on finding the exact active subspace.
It allows to optimize over smaller perturbations as well, whereas current algorithms usually don't take these into consideration.
\item I analyse the relationship between improved log-likelihood and angle difference. 
I see that it is not surprising that the angle-difference between the real projection matrix, and the found projection matrix is not closely coupled to the log-likelihood.
However, improving over the log-likelihood can reduce the angle-difference in many cases.
\end{enumerate}

\section{Future work}

Future work could incorporate the synthesis of different methods, including additive GPs and Tripathy's method.
Although tripathy's method is unbeaten in identifying the active subspace dimension, heuristics could be easily implemented to speed up the calculation time.
One of the most important aspects is hyperparameter tuning for the GP models itself.
These can make or break the identification of the subspace.
Choosing bad hyperparameters does not allow us to effectively compare different methods.
In future, it would be beneficial to address this issue to a stronger extent.
Recalculating the search space for each new point may be too time consuming, with which REMBO would still be considered the state of the art in terms of Bayesian Black Box Optimization.
 



