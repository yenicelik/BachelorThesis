%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Related Work}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

This section will cover approaches taken so far to solve the problem.
We will present algorithms that solve the approach.
We will shortly discuss the effectiveness of this algorithm based on the dataset that the authors work on.

Some convention that we use is the following:

\begin{enumerate}
\item $f$ is the function that we want to approximate
\item $g$ and any subscripted or superscripted derivative of $g$ is a component that we use to approximate $f$.
\item Anything that has a "hat" on (caret symbol \^ ), refers to the empirical estimate. 
\end{enumerate}

We will focus on the three points: 
% Identification of the subspace)

\section{Projection matrix based algorithms}

We start with enumerating some existing algorithms that are based on projecting the optimization domain.
Generally, for this family of algorithms, there is always a term $f(x) \sim g(Ax)$ present, where the properties and generation of $A$ are algorithmic specific.

\subsection{Active learning of linear subspace}

\begin{algorithm}
\caption{Simultaneous active learning of functions and their linear embeddings (pseudocode) :: Active learning of linear subspace CITE GARNETT 2013}

\begin{algorithmic} 
\REQUIRE $d, D;$ kernel $\kappa$, mean function $\mu$; prior $p(R)$ 
\STATE $X \leftarrow \emptyset$
\STATE $Y \leftarrow \emptyset$

\WHILE{budget not depleted}
\STATE $ q(R) \leftarrow \text{LAPLACEAPPROX}( p(R | X, Y, \kappa, \mu) ) $
\STATE $ q(f) \leftarrow  APPROXMARGINAL( p(f | R), q(R)) $
\STATE $ x_* \leftarrow OPTIMIZEUTILITY( q(f), q(R) )$
\STATE $ y \leftarrow OBSERVE( f( x_* ) ) $
\STATE $ X \leftarrow [X; x_*] $
\STATE $ Y \leftarrow[Y; y_*] $
\ENDWHILE

\RETURN $q(R), q(f)$
\end{algorithmic}

\end{algorithm}

\citep{Garnett2013} Assume $f$ depends only on $ x := uR^T $ with $ R \in \mathbf{R}^{d \times D}$ where $d << D$. 
Learn an algorithm that learns $g(u) = f(x)$ and $R$.

The proposed algorithm takes the following steps to learn $g$ and $R$:

\begin{enumerate}
\item Create a probability distribution over possible embeddings to learn $R$ (Laplace approximation).
\item We use the calculated embeddings to create a posterior probability distribution over $f$.
\item Perform active selection over all possible points.
\end{enumerate}

The choice of next point is done using Bayesian Active Learning by disagreement, where the utility function is the expected reduction in entropy (equal to the mutual information), as opposed to uncertainty sampling, which simply minimizes the entropy. \\

The metrics used in this paper are negative log-likelihoods for the test points, and the mean SKLD (nat) between approximate and true posteriors.
The proposed method always outperforms the naive MAP method.
Tests are conducted on a real, and synthetic dataset with up to $D = 318$ and selecting $N = 100$ observations. 

% TODO : Go more into detail here, potentially read the paper

\subsection{High dimensional Gaussian bandits}

\citep{Djolonga2013} Assume there exists a function $g : \mathbf{R}^k \implies [0, 1]$ and a matrix $A \in \mathbf{R}{d \times D}$ with orthogonal rows, such that $f(x) = g(Ax) $. Assume $g \in \mathcal{C}^2$. 
Assume that $B = \mathbf{B}^D (1 + \epsilon ) $.
We want to maximize $f: B \implies [0, 1] $.\\

\begin{algorithm}
\caption{The SI-BO algorithm CITE DJOLONGA2013}

\begin{algorithmic} 
\REQUIRE $m_X, m_{\Phi}, \lambda, \epsilon, k$, oracle for the function $f$, kernel $\kappa$ 

\STATE $C \leftarrow m_X $ samples uniformly from $\mathbb{S}^{d-1}$

\FOR{$ i \leftarrow 1$ to $m_X$}
\STATE $\Phi_i \leftarrow m_{\Phi}$ samples uniformly from $\{ -\frac{1}{\sqrt{m}}, \frac{1}{\sqrt{m}} \}^k$
\ENDFOR

\STATE $ y \leftarrow $ (compute using Equation 1 -- INSERT Eq 1 here, or create a summary of all important equations here)

\STATE select $z_i$ according to a UCB acquisition function, evaluate $f$ on it, and add it to the datasamples found so far

\end{algorithmic}

\end{algorithm}

The SI-BO algorithm has a two-step approach:
1.) subspace identification.
2.) Bayesian Optimization on the learned subspace.


\subsection{Random embeddings (REMBO)}
\citep{Wang2013} Let $x \in \mathbb{R}^D$ and $y \in \mathbb{R}^d$. Assume, that $f(x) = f(Ax)$. We can generate $A \in \mathbb{R}^{D \times d}$ by randomly generating this matrix.

\subsection{Applications to high-dimensional uncertainty propogation}
\citep{Tripathy} Assume $f(x) \approx g( \mathbf{W}^T y)$ where $ \mathbf{W} \in \mathbb{R}^{D \times d} $ and $D >> d$.
We assume that $ \mathbf{W} $ is orthogonal.

This algorithm does not require gradient-information (thus, easier to implement, and robust to noise).
The standard-deviation, kernel parameters and  $ \mathbf{W} $ can be found iteratively.
First we fix $ \mathbf{W} $, and optimize over the standard-deviation, kernel parameters.
Then we fix the standard-deviation, kernel parameters. and optimize over $ \mathbf{W} $.
We repeat this procedure until the change of the log-likelihood between iterations is below some $ \epsilon_l $.\\

A more detailed description:

The quantities of interest are:

\begin{align}
\mu_f &= \int f(x) p(x) dx \\
\sigma^2_f &= \int ( f(x) - \mu_f )^2 p(x) dx \\
f \sim p(f) &= \int \delta( f - f(x) ) p(x) dx
\end{align}

We further assume that the response surface has the form of 

$$
f(x) \approx g(W^T x)
$$.

Because all matrices yield identical approximations, one can focus on orthogonal approximations.

For the family of orthogonal matrices of dimension $d \times D$, we write $\mathbf{W} \in V_d(\mathbb{R}^D) $.
This quantity is also known as the Stiefel manifold. \\

The paper focuses on identifying the low dimensional map $g( \dot )$, the effective dimensionality $d$ and the orthogonal matrix $W$.\\

We determine the hyperparameters by optimizing over the following loss function, where $Q$ are the input samples, $t$ are the corresponding output samples, $\theta$ are the ... , and $t$ are the .

$$
\mathcal{L} (\theta, s_n; Q, t) = \log p(t | Q, \theta, s_n)
$$

where we find the individual variables by maximizing this log-likelihood. \\

The authors introduce a new kernel function 

\begin{equation}
k_{AS} : \mathbb{R}^D \times \mathbb{R}^D \times V_d(\mathbb{R}^D) \times \phi -> \mathbb{R} \\
\end{equation}
\text{where the kernel has the form}
\begin{equation}
k_{AS} (x, x'; W, \phi) = k_d(W^T x, W^T x'; \phi)
\end{equation}

Within the paper, we use the following kernel function

\begin{align}
1
\end{align}

\subsubsection{Optimize $W \in V_d(\mathbb{R}^D)$ and keep $\phi$ and $s_n^2$ fixed}

For this subsection, redefine the loss function as ($\phi$ are the hyperparameters of the covariance function):

(We instantiate $\theta$ with $W$)
\begin{align}
F(W) &:= \mathcal{L}(W, s_n; X, y) \\
& = \log p(y | X, W, s_n) \\
& =  -\frac{1}{2} (y - m)^T (K + s_n^2 I_N)^{-1} (y - m) -\frac{1}{2} \log|K + s_n^2 I_N| -\frac{N}{2} \log 2 \pi   \\
\end{align}

where $\phi, s_n; X, y$ are fixed and $m$ is the prior mean function.

The derivative of this function $F$ with respect to the weights-matrix is:

\begin{align}
\nabla_W F(W) &:= \nabla_W \mathcal{L}(W, s_n; X, y) \\
& = \frac{1}{2} \text{tr} \left[ \{ (K + s_n^2 I_N)^{-1} (y-m) \left( (K + s_n^2 I_N)^{-1} (y-m) \right)^T - (K + s_n^2 I_N)^{-1} \} \nabla_W (K + s_n^2 I_N) \right]
\end{align}

both these functions depend on the kernel $K$, and it's derivative $\nabla_W K$.

We use the 32-Matern kernel function (for two vectors $a$ and $b$) with derivative:

\begin{align}
K(a,  b, \theta) = s^2 \left( 1 + \sqrt{3} \sum_{i=1}^l \frac{(a_i - b_i)^2}{ \textit{l}_i} \right) exp\left( - \sqrt{3} \sum_{i=1}^l \frac{(a_i - b_i)^2}{ \textit{l}_i} \right)
\end{align}

\begin{align}
\nabla_W K(a,  b, \theta) = s^2 \left( 1 + \sqrt{3} \sum_{i=1}^l \frac{(a_i - b_i)^2}{ \textit{l}_i} \right) exp\left( - \sqrt{3} \sum_{i=1}^l \frac{(a_i - b_i)^2}{ \textit{l}_i} \right)
\end{align}

where $s, l_1, \ldots, l_l $ are hyper-parameters included within $\theta$

We optimize over this non-convex optimization problem by using a gradient-descent technique that optimizes over the Stiefel manifold (class of orthogonal matrices).



\subsubsection{Identification of active subspace dimension }



\section{Algorithms that exploit additive substructures}

We turn our attention to algorithms that assume that the function can be decomposed into a summation over subfunctions, such that
$ f(x) \sim g_0(x) + g_1(x) + \ldots g_2(x) $ where each $g_i$ may operate only on a subset of dimensions of $x$.

\subsection{Independent additive structures within the target function}
\citep{Gardner2017} Assume that $f(x) = \sum_{i=1}^{ |P| } f_i (x[P_i] )$, i.e. $f$ is fully additive, and can be represented as a sum of smaller-dimensional functions $f_i$, each of which accepts a subset of the input-variables.
The kernel also results in an additive structure: $f(x) = \sum_{i=1}^{ |P| } k_i (x[P_i], x[P_i])$.
The posterior is calculated using the Metropolis Hastings algorithm.
The two actions for the sampling algorithm are 'Merge two subsets', and 'Split one set into two subsets'.
$k$ models are sampled, and we respectively approximate $p(f_* | D, x^*) = \frac{1}{k} \sum_{j=1}^{k} p( f(x^* | D, x, M_j) )$, where $M_j$ denotes the partition amongst all input-variables of the original function $f$.

\section{Further approaches}

\subsection{Elastic Gaussian Processes}
\citep{Rana2017} Use a process where the space is iteratively explored.
The key insight here is that with low length-scales, the acquisition function is extremely flat, but with higher length-scales, the acquisition function starts to have significant gradients.
The two key-steps is to 1.) additively increase the length-scale for the gaussian process if the length-scale is not maximal and if $|| x_{init} - x^* || = 0$.
And 2.) exponentially decrease the length-scale for the gaussian process if the length-scale is below the optimum length-scale and if $|| x_{init} - x^* || = 0$.


\subsection{Bayesian Optimization using Dropout}
\citep{Li2018} propose that the assumption of an active subspace is restrictive and often not fulfilled in real-world applications.
They propose three algorithms, to iteratively optimize amongst certain dimensions that are not within the $d$ 'most influential' dimensions: 1.) Dropout Random, which picks dimensions to be optimized at random, 2.) Dropout copy, which continuous optimizing the function values from the found local optimum configuration, and 3.) which does method 1. with probability $p$, and else method 2.
The $d$ 'most influential' dimensions are picked at random at each iteration.
