%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Fields of Improvement}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

\section{Shortcomings of current methods}
I will enumerate select models from the section "related work", and will shortly discuss what the shortcomings of these models are:

\paragraph{REMBO} is a purely optimizational algorithm, which finds optimizations in lower dimensions

\begin{itemize}

\item \textbf{Suitable choice of the optimization domain:} REMBO is not purely robust, as there is a considerable chance that no suitable subspace will be found. 
Empirically, the choice of the optimization domain heavily affects the duration and effectiveness of the optimization.
I have found that the proposed optimization domain $ \left[ -\sqrt{d}, \sqrt{d}  \right]^d $ is not well chosen for smaller environments, such as the Camelback function embedded in 5 dimensions.
In any case, this is a very sensitive hyperparameter

\item \textbf{Identification of subspace:} In some settings, including optimization with safety constraints, knowing the subspace that the model projects to is advantageous. 
REMBO is an implicit optimizer, in that it does not find any subspace, but optimizes through a randomly sampled matrix.

\item \textbf{Probability of failure:} REMBO has a relatively high probability of failure. 
The authors propose that restarting REMBO multiple times would allow for a good optimization domain to be found, which leads to interleaved runs. 

\end{itemize}

\paragraph{Active subgradients} can be a viable option if we have access to the gradients of the problem, from which we can learn the active subspace projection matrix in the manner by using that gradient matrices.

\begin{itemize}

\item \textbf{Access to gradients:} For optimization algorithms, the function we want to optimize over is usually a black-box function.
Practically, most black-box functions don't offer access to gradient information.
To approximate the gradients using sampled points, this would require a high number of datapoints per dimension.
In addition to that, these points would have to be evenly distributed, such that the gradients can be effectively estimated for more than one region.

\item \textbf{Robustness to noise:} According to \citep{Tripathy}, methods that approximate gradients and use this gradient information to create subspace projectsion are very sensitive to noise.
Depending on the application, this can make the algorithm ineffective as it is not robust to small variations in the response surface.

\end{itemize}

Given the nature of real-world data, approximating the active subspace using the gradients of the data-samples is thus not a robust, and viable option.

\paragraph{Tripathy's method} argues that it is more robust to real-world noise. 
It also does not rely on gradient information of the response surface.
Tripathy's method allows for a noise-robust way to identify the active subspace.

\begin{itemize}

\item \textbf{Duration of optimization:} In practice, Tripathy's method takes a long time, especially if the dimensions, or the number of data-points are high. This is due to the high number of matrix multiplications. 
Especially for easier problems, it is often desirable if the running time of optimizing for the next point is in a few minutes or seconds, rather than hours.

\item \textbf{Efficiency:} In practice, Tripathy's method relies on a high number of restarts.
From our observations, the number of steps to optimize the orthogonal matrix becomes relevant as the number of dimensions grow.
Given the nature of accepting any starting point, it does not allow for a very efficient way to search for the best possible projection matrix.
A more efficient way to search all possible matrices - by incorporating heuristics for example - would be desirable.

\item \textbf{Insensitive to small perturbations:} Although Tripathy's model finds an active subspace, it completely neglects other dimensions which could allow for small perturbations to allow for an additional increase the global optimum value.
Although we can control to what extent small perturbations should be part of the active subdimension, one usually wants to choose a significant cutoff dimension, but still incorporate additional small perturbations without sacrificing the effectiveness of the projection.

\end{itemize}

\section{Method of measuring improvements}
In the following sections, we will discuss and show how we can improve on the shortcomings of the above methods.
Because practicality is important in our method, we will both use synthetic functions to measure the efficiency of our method, but also real datasets.
For real datasets, we want to see if the
\textbf{???} log likelihood improves.
I will use a real dataset of an electron accelerator.
I will increasingly train the GP model on a number of data-points.
As more data-points are added to the GP, the log likelihood of the test points should have the tendency to increase.\\

Besides that, we offer the following possibilities to check how well our model does.

\begin{itemize}
\item Test if the expectation $$ E[ f(A x) - \hat{f}(\hat{A} x) ] $$ decreases / approaches zero (for methods that identify a projection matrix).
Often, the root mean square error is a good empirical approximate of this quantity:
\begin{equation}
RMSE = \sqrt{ \frac{1}{T} (\sum_{t=1}^{T} f(A x_t) - \hat{f}(\hat{A} x_t))^2 }
\end{equation}

\item For optimization problems, one is often interested in the quantity of cumulative regret.
Regret is defined as the difference between the best found function value so far, minus the function value chosen at this timestep $t$ \citep{RegretDef}.

\begin{equation}
R_T = \frac{1}{T} \sum_{t=1}^{T} \max_x f(x) - f(x_t)
\end{equation}

The cumulative regret sums all the entire episode of the run.
This is a measure of how fast an optimizer can learn the optima of a function.

\item Check if the test log-likelihood decreases for functions that are provided by a finite number of data-points.
\item Check if the regret for the methods at hand are competitive, for synthetic functions.
\item Check if the angle between the real projection matrix, and the found projection matrix decreases, as given in \citep{AngleMeasurement}.
\end{itemize}


\subsection{Synthetic Datasets} \label{syntheticFunction}
\paragraph{5 dimensional function with 2 dimensional linear embedding}

One can evaluate synthetic functions at any point, and immediately get a regret value.
The following synthetic functions cover different use cases.

\paragraph{2D to 1D}: A simple Parabola which is embedded in a 2D space.
This function is supposed to check that the complexity of the model does not hinder discovering simpler structures - in other words, the model complexity should still allow for finding simpler embeddings and functions.
\paragraph{5D to 2D}: The Camelback function which is embedded in a 5D space.
This checks if simple models can be found within certain, higher dimensional spaces.
\paragraph{5D to 2D}: A simple exponential function which is enclosed in a sinusoidal function (with decreasing amplitude), which is embedded in a 5D space.
This function measures if small perturbations are covered by the more complicated model (our proposed model).

\subsection{Real Datasets}
It is more difficult to test algorithms on real datasets, as one cannot test on a metric such as regret.
However, one can train the GP on a training set, and then compare the log-likelihood on a test set.

\paragraph{SwissFEL dataset}
\dots and some more 


