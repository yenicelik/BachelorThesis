\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Projection matrix based algorithms}{7}{section.2.1}}
\citation{Garnett2013}
\citation{Garnett2013}
\citation{Garnett2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Active learning of linear subspaces}{8}{subsection.2.1.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simultaneous active learning of functions and their linear embeddings (pseudocode) :: Active learning of linear subspace \citep  {Garnett2013}\relax }}{8}{algorithm.1}}
\citation{Wang2013}
\citation{Wang2013}
\citation{Wang2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Random embeddings (REMBO)}{9}{subsection.2.1.2}}
\citation{Wang}
\citation{Wang2013}
\citation{Wang2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Parabola Original\relax }}{10}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gull}{{2.1}{10}{Parabola Original\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Source \citep  {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathbf  {X}$, while the thicker red line illustrates the 1D constrained space $\mathbf  {Y}$. Note that if $A \times y$ is outside of $\mathbf  {X}$, it is projected onto $\mathbf  {X}$ using a convex projection. The set $\mathbf  {Y}$ must be chosen large enough so that the projection of its image, $A \times y $ with $y \in \mathbf  {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box. \relax }}{10}{figure.caption.6}}
\newlabel{fig:animals}{{2.2}{10}{Source \citep {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathbf {X}$, while the thicker red line illustrates the 1D constrained space $\mathbf {Y}$. Note that if $A \times y$ is outside of $\mathbf {X}$, it is projected onto $\mathbf {X}$ using a convex projection. The set $\mathbf {Y}$ must be chosen large enough so that the projection of its image, $A \times y $ with $y \in \mathbf {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box. \relax }{figure.caption.6}{}}
\citation{RemboExtension}
\citation{Tripathy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Parabola Original\relax }}{11}{figure.caption.7}}
\newlabel{fig:gull}{{2.3}{11}{Parabola Original\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Source \citep  {Wang2013}: This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional function\IeC {\textquoteright }s optimized value $x^*$. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }}{11}{figure.caption.7}}
\newlabel{fig:animals}{{2.4}{11}{Source \citep {Wang2013}: This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional functionâ€™s optimized value $x^*$. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Applications to high-dimensional uncertainty propagation}{11}{subsection.2.1.3}}
\citation{StiefelBayesianInference}
\citation{StatisticsStiefelIntro}
\citation{StiefelNonparametric}
\@writefile{toc}{\contentsline {paragraph}{I now proceed with a more formal description of the algorithm.}{12}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{The Matern32 Kernel}{12}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Overview of the algorithm}{13}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{Step 1.: Determine the active projection matrix W}{13}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{Step 2.: Optimizing over GP noise variance and the kernel hyperparameters}{14}{section*.11}}
\citation{Gardner2017}
\@writefile{toc}{\contentsline {subsubsection}{Additional details}{15}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{Identification of active subspace dimension }{15}{section*.13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Algorithms that exploit additive substructures}{15}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Independent additive structures within the target function}{15}{subsection.2.2.1}}
\citation{Rana2017}
\citation{Djolonga2013}
\citation{Djolonga2013}
\citation{Djolonga2013}
\citation{CevherSubspaceIdentificationKrause}
\citation{Li2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Additional approaches}{16}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Elastic Gaussian Processes}{16}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}High dimensional Gaussian bandits}{16}{subsection.2.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The SI-BO algorithm \citep  {Djolonga2013}\relax }}{16}{algorithm.2}}
\citation{KernelGibbsSampler}
\citation{VirtualVsReal}
\citation{SensorPlacement}
\citation{BatchedBO}
\citation{GPforML}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Bayesian Optimization using Dropout}{17}{subsection.2.3.3}}
\@setckpt{04_Chapter2/chapter2}{
\setcounter{page}{18}
\setcounter{equation}{15}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{7}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{28}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@unique}{17}
\setcounter{ALC@line}{6}
\setcounter{ALC@rem}{6}
\setcounter{ALC@depth}{0}
\setcounter{section@level}{2}
}
