\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{9}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Projection matrix-based algorithms}{9}{section.3.1}}
\citation{Garnett2013}
\citation{Garnett2013}
\citation{Garnett2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Active learning of linear subspaces}{10}{subsection.3.1.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simultaneous active learning of functions and their linear embeddings (pseudocode) :: Active learning of linear subspace \citep  {Garnett2013}\relax }}{10}{algorithm.1}}
\citation{Wang2013}
\citation{Wang2013}
\citation{Wang2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Random embeddings (REMBO)}{11}{subsection.3.1.2}}
\citation{Wang2013}
\citation{Wang2013}
\citation{Wang2013}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Parabola Original\relax }}{12}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gull}{{3.1}{12}{Parabola Original\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Source \citep  {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathbf  {X}$, while the thicker red line demonstrates the 1D constrained space $\mathbf  {Y}$. Note that if $A \times y$ is outside of $\mathbf  {X}$, it is projected onto $\mathbf  {X}$ using a convex projection. The set $\mathbf  {Y}$ must be chosen large enough so that the projection of its image, $A \times y $ with $y \in \mathbf  {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box. \relax }}{12}{figure.caption.3}}
\newlabel{fig:animals}{{3.2}{12}{Source \citep {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathbf {X}$, while the thicker red line demonstrates the 1D constrained space $\mathbf {Y}$. Note that if $A \times y$ is outside of $\mathbf {X}$, it is projected onto $\mathbf {X}$ using a convex projection. The set $\mathbf {Y}$ must be chosen large enough so that the projection of its image, $A \times y $ with $y \in \mathbf {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box. \relax }{figure.caption.3}{}}
\citation{RemboExtension}
\citation{Tripathy}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Parabola Original\relax }}{13}{figure.caption.4}}
\newlabel{fig:gull}{{3.3}{13}{Parabola Original\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Source \citep  {Wang2013}: This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional function\IeC {\textquoteright }s optimized value $x^*$. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }}{13}{figure.caption.4}}
\newlabel{fig:animals}{{3.4}{13}{Source \citep {Wang2013}: This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional functionâ€™s optimized value $x^*$. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}GPs with builtin dimensionality reduction}{13}{subsection.3.1.3}}
\citation{StiefelBayesianInference}
\citation{StatisticsStiefelIntro}
\citation{StiefelNonparametric}
\@writefile{toc}{\contentsline {paragraph}{Formal description of the algorithm.}{14}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{The Matern32 Kernel}{14}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Overview of the algorithm}{15}{subsection.3.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{Step 1.: Determine the active projection matrix W}{15}{section*.7}}
\newlabel{Eq:LogLikelihoodF}{{3.4}{15}{Step 1.: Determine the active projection matrix W}{equation.3.1.4}{}}
\newlabel{Eq:TauFunction}{{3.10}{16}{Step 1.: Determine the active projection matrix W}{equation.3.1.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Step 2.: Optimizing over GP noise variance and the kernel hyperparameters}{16}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{Additional details}{16}{section*.9}}
\citation{Gardner2017}
\citation{Rana2017}
\@writefile{toc}{\contentsline {subsubsection}{Identification of active subspace dimension }{17}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Algorithms that exploit additive substructures}{17}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Independent additive structures within the target function}{17}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Additional approaches}{17}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Elastic Gaussian Processes}{17}{subsection.3.3.1}}
\citation{Djolonga2013}
\citation{Djolonga2013}
\citation{Djolonga2013}
\citation{Djolonga2013}
\citation{CevherSubspaceIdentificationKrause}
\citation{Li2018}
\citation{KernelGibbsSampler}
\citation{VirtualVsReal}
\citation{SensorPlacement}
\citation{BatchedBO}
\citation{GPforML}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}High dimensional Gaussian bandits}{18}{subsection.3.3.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The SI-BO algorithm \citep  {Djolonga2013}\relax }}{18}{algorithm.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Bayesian Optimization using Dropout}{18}{subsection.3.3.3}}
\@setckpt{04_Chapter2/chapter2}{
\setcounter{page}{19}
\setcounter{equation}{12}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{3}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{8}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{24}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@unique}{17}
\setcounter{ALC@line}{6}
\setcounter{ALC@rem}{6}
\setcounter{ALC@depth}{0}
\setcounter{section@level}{2}
}
