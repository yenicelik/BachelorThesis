\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Garnett2013}
\citation{Garnett2013}
\citation{Garnett2013}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Projection matrix based algorithms}{7}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Active learning of linear subspace}{7}{subsection.2.1.1}}
\citation{Djolonga2013}
\citation{Djolonga2013}
\citation{Djolonga2013}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simultaneous active learning of functions and their linear embeddings (pseudocode) :: Active learning of linear subspace \citep  {Garnett2013}\relax }}{8}{algorithm.1}}
\citation{CevherSubspaceIdentificationKrause}
\citation{Wang2013}
\citation{Wang2013}
\citation{Wang2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}High dimensional Gaussian bandits}{9}{subsection.2.1.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The SI-BO algorithm \citep  {Djolonga2013}\relax }}{9}{algorithm.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Random embeddings (REMBO)}{9}{subsection.2.1.3}}
\citation{Wang}
\citation{Wang2013}
\citation{Wang2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Parabola Original\relax }}{10}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gull}{{2.1}{10}{Parabola Original\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Taken from \citep  {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathsf  {X}$, while the thicker red line illustrates the 1D constrained space $\mathsf  {Y}$. Note that if $Ay$ is outside $\mathsf  {X}$, it is projected onto $\mathsf  {X}$. The set $\mathsf  {Y}$ must be chosen large enough so that the projection of its image, $A\mathsf  {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box). \relax }}{10}{figure.caption.6}}
\newlabel{fig:animals}{{2.2}{10}{Taken from \citep {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathsf {X}$, while the thicker red line illustrates the 1D constrained space $\mathsf {Y}$. Note that if $Ay$ is outside $\mathsf {X}$, it is projected onto $\mathsf {X}$. The set $\mathsf {Y}$ must be chosen large enough so that the projection of its image, $A\mathsf {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box). \relax }{figure.caption.6}{}}
\citation{RemboExtension}
\citation{Tripathy}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Parabola Original\relax }}{11}{figure.caption.7}}
\newlabel{fig:gull}{{2.3}{11}{Parabola Original\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Taken from \citep  {Wang2013} Fig. 2: This function in D=2 dimesions only has d=1 effective dimension: the vertical axis indicated with the word important on the right hand side figure. Hence, the 1-dimensional embedding includes the 2-dimensional function\IeC {\textquoteright }s optimizer. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }}{11}{figure.caption.7}}
\newlabel{fig:animals}{{2.4}{11}{Taken from \citep {Wang2013} Fig. 2: This function in D=2 dimesions only has d=1 effective dimension: the vertical axis indicated with the word important on the right hand side figure. Hence, the 1-dimensional embedding includes the 2-dimensional functionâ€™s optimizer. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Applications to high-dimensional uncertainty propogation}{11}{subsection.2.1.4}}
\citation{StiefelBayesianInference}
\citation{StatisticsStiefelIntro}
\citation{StiefelNonparametric}
\@writefile{toc}{\contentsline {paragraph}{I now proceed with a more detailed description of the algorithm.}{12}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{Kernel used}{12}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{Step 1.: Determine the active projection matrix W}{13}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{Step 2.: Optimizing over GP noise variance and the kernel hyperparameters}{14}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{Additional details}{14}{section*.12}}
\@writefile{toc}{\contentsline {subsubsection}{Identification of active subspace dimension }{14}{section*.13}}
\citation{Gardner2017}
\citation{Rana2017}
\citation{Li2018}
\citation{KernelGibbsSampler}
\citation{VirtualVsReal}
\citation{SensorPlacement}
\citation{BatchedBO}
\citation{GPforML}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Algorithms that exploit additive substructures}{15}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Independent additive structures within the target function}{15}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Additional approaches}{15}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Elastic Gaussian Processes}{15}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Bayesian Optimization using Dropout}{15}{subsection.2.3.2}}
\@setckpt{04_Chapter2/chapter2}{
\setcounter{page}{17}
\setcounter{equation}{15}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{6}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{27}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALC@unique}{17}
\setcounter{ALC@line}{6}
\setcounter{ALC@rem}{6}
\setcounter{ALC@depth}{0}
\setcounter{section@level}{2}
}
