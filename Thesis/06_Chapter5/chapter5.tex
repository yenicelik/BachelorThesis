%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Fourth Chapter **********************************
%*******************************************************************************
\chapter{Model Design and Extensions to the state of the art}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

Given the fields of improvements in the above section, we now propose an algorithm which addresses the majority of the issues mentioned in the above section.
We will first present that algorithm, and then points out, as to why each individual concern is addressed.

\section{The BORING Algorithm}

We propose the following algorithm, called BORING. \textbf{BORING} stands for \textbf{B}ayesian \textbf{O}ptimization using \textbf{R}EMBO and \textbf{I}de\textbf{N}tifyable subspace \textbf{G}eneration.

The general idea of boring can be captured in one formula, where $f$ stands for the real function that we wish to approximate, and any subsequent function annotated by $g$ refers to a component of the right hand side, which approximates the function $f$.

\begin{equation}
f(x) \approx g_0(A x) + \sum_{i \in \mathbb{Z}^+}^{q} g_i( A^{\bot} x)_i )
\label{eq:dimRedEquation}
\end{equation} \\

Where the following variables have the following meaning
\begin{itemize}
\item $A$ is the active subspace projection (an element of the stiefel manifold) learned through our algorithm, using Algorithm 1
\item $A^{\bot}$ is an matrix whose subspace is orthonormal to the projection of $A$.
We generate $A^{\bot}$ using Algorithm 2.
\item The subscript $i$ in the right additive term dneotes that we view each output dimension (of $dot(A^{\bot}, X)$ individually and as independent.

\end{itemize}

We will no proceed with describing the algorithm in more detail.

\subsection{Algorithm Description}

\subsubsection{General Idea of the algorithm}

We propose a novel method which is based on additive GPs, where we use different kinds of kernels.
We want to calculate $g_i$ and $f$ within that \ref{eq:dimRedEquation}, such that the maximum likelihood of the data we have accumulated so far.
 Because the term can be written as a sum of expressions, we can maximize each summand individually, which will lead to maximizing the entire expression (after we have calculate the active subspace). \\
 
 The following few steps are applied after a "burn-in" phase, in which we use REMBO to acquire new points (as this is an efficient form of random sampling).
 We take $ \sqrt{d} $ as the dimensionality of the space $Y$ in which we search (heuristic!!).
 
 \begin{enumerate}
 \item Calculate the active subspace using the algorithm from 
 \item Calculate an appropriate pair of vectors $v_1, \ldots, v_{n-{q}}$, where each vector is orthonormal to every other vector in $A$.
 \item Maximize the GP for each individual expression of the space orthogonal to $A$ (as given by $V x$) individually.
 \end{enumerate}
 
 This fights the curse of dimensionality, as we can freely choose $q \geq d_e$ to set the complexity of the second term.
 This, thus allows for smaller perturbations in the space orthogonal to $A$ to occur.

\begin{algorithm}[H]
\caption{BORING Alg. 1 - Bayesian Optimization using BORING}

\begin{algorithmic} 
\STATE $X \leftarrow \emptyset$
\STATE $Y \leftarrow \emptyset$

\COMMENT{Burn in rate - don't look for a subspace for the first 50 samples}
\STATE $i \leftarrow 0$
\WHILE{i < 50}
\STATE $i++$
\STATE $x_* \leftarrow $ argmax$_x$ acquisitionFunction$(dot(Q^{\bot}, x) )$ using standard UCB over the domain of $X$.
\STATE Add $x_*$ to $X$ and $ f(x_*)$ to $Y$.
\ENDWHILE

\WHILE{we can choose a next point}
% TODO: add an if statement that the active subspace is re-calculated every now and then
\STATE $A, d \leftarrow $ Calculate active subspace projection using Algorithm 2 from the paper by Tripathy.
\STATE $A^{\bot} \leftarrow $ Generate passive subspace projection using Algorithm 3.
\STATE TODO: CHECK DIMENSIONS IN THE CODE!!! (If concat is in the correct dimension!)
\STATE $Q \leftarrow $ colwiseConcat( $[A, A^{\bot}]$ ) 
\STATE $gp \leftarrow GP( $dot$( Q^T, X), Y)$
\STATE kernel $\leftarrow$ activeKernel + $\sum_i^{q}$ passiveKernel$_i$ 
\COMMENT{For this one, maybe be more explicit with how to set the kernels (as each kernel addresses a different number of dimensions)}
\STATE $x_* \leftarrow $ argmax$_x$ acquisitionFunction$(dot(Q^{\bot}, x) )$ using Equations (REFERENCE HERE)
\STATE Add $x_*$ to $X$ and $ f(x_*)$ to $Y$.
\ENDWHILE

\RETURN $q(R), q(f)$
\end{algorithmic}

\end{algorithm}


We find the active projection matrix using the following algorithm, which is identical to the procedure described in "CITE TRIPATHY".
We then generate a matrix $ A^{\bot} $ by using the following procedure.

\subsubsection{Finding a basis for the passive subspace (a subspace orthogonal to the active subspace)}

\begin{equation}
A = 
\begin{bmatrix}
 \vdots & \vdots & & \vdots \\
 a_1 & a_2 & ... & a_{d_e} \\
 \vdots & \vdots & & \vdots
\end{bmatrix}
\label{eq:maximalEmbedding}
\end{equation}

Given that we choose a maximal lower dimensional embedding (maximising the log-likelihood of the embedding for the given points), some other axes may be disregarded.
However, the axes that are disregarded may still carry information that can make search faster or more robust.

To enable a trade-off between time and searchspace, we propose the following mechanism.

%% TODO I think it's the rowspace.. not the columnspace!
%% Potentially use this to generate these vectors: https://stackoverflow.com/questions/33658620/generating-two-orthogonal-vectors-that-are-orthogonal-to-a-particular-direction

Assume we have found the maximal embedding \ref{eq:maximalEmbedding}.
Then we have found the active subspace, which is characterizable through it's first few column vector $  a_1, a_2, ..., a_{d_e} $.
However, as said before, we also want to address the subspace which is not addressed by the maximal embedding, which we will refer to \textit{passive subspace}.
This passive subspace can be characterized by the set of vectors, that are all orthogonal to all other column vectors in $A$, i.e. the space orthogonal to the projection of $A$.

As such, we define the entire span of the given vectorspace as:

\begin{equation}
S = 
\begin{bmatrix}
A & V
\end{bmatrix}
\label{eq:entireSubspace}
\end{equation}

where $V$ describes the matrix that is orthogonal to the columnspace of $A$.
For this, $V$ consists of any basis vectors that are orthogonal to all other vectors in $A$.\\

We can generate these vectors by taking a random vector, and applying gram schmidt.
We repeat the above procedure if the norm of the resulting vector is smaller than a given threshold.
 (One could also maybe use fourier, then add another fourier axis, and then go back again to have a nice basis?). \\

\begin{algorithm}[H]
\caption{BORING Alg. 3 - generate orthogonal matrix to A(A, n) }

\begin{algorithmic} 
\REQUIRE $A$ a matrix to which we want to create $A^{\bot}$ for; $n$, the number of vectors in $A^{\bot}$.

\STATE normedA $ \leftarrow $ normalize each column of $A$
\STATE $Q \leftarrow$ emptyMatrix()
\COMMENT{ The final concatenated $Q$ will be $A^{\bot}$. }
\FOR{i = 1,...,n}
\STATE $i \leftarrow 0$ 
\WHILE{i < 50}
\STATE i++
\STATE $q_i \leftarrow $ random vector with norm 1
\STATE newBasis = apply gram schmidt single vector( $[A, Q], q_i$ ) 

\IF{ dot(normed$A^T$, newBasis) $\approx \mathbf{0}$ and $|$ newBasis $|$ $> 1e-6$}
\STATE $Q \leftarrow$ colwiseConcatenate( $(Q, $ newBasis)
\STATE break
\ENDIF
\ENDWHILE                
\ENDFOR

\RETURN $Q$
\end{algorithmic}
\end{algorithm}

\subsubsection{Additive UCB acquisition function}

Because we decompose the function into multiple additive components, we need to adapt the mean and variance computation accordingly, as is described in (CITE THE EPFL PAPER). \\

\begin{align}
\mu_{t-1}^{(j)} &= k^j(x_*^{(j)}, X^j)\Delta^{-1}y \\
\left( \sigma_{t-1}^{(j)} \right)^2 &= k^j(x_*^{j}, x_*^{j}) - k^j(x_*^j, X^{(j)}) \Delta^{-1} k^j(X^{(j)}, x_*^j)
\end{align}

where $k(a, b)$ is the piecewise kernel operator for vectors or matrices $a$ and $b$ and $\Delta = k(X, X) + \eta I_n$.
Because we only use a single GP with multiple kernels (where each kernel handles a different dimension of $dot(Q^T, x)$), we have $k^{j=1, \ldots, q+1}$ kernels (the $+1$ comes from the first kernel being the kernel for the active subspace). 


\section{Additive Stiefel projections - Our proposed improvement to existing methods}

We model the function $f$, which we approximate through functions $g_{i}$ as follows:

We propose the following training method:

\begin{enumerate}
\item We first approximate the function 
$$ f(x) \sim g_1(A_1, x) $$.
We decide the dimensionality of the matrix $A_1$ by deciding a 'cutoff dimension' as proposed by Tripathy et al. in their Algorithm 4.
$g_1$ is a gaussian process, and $A_1$ is a learned matrix.

\item We then expand this approximated term to
$$ f(x) \sim g_1(A_1, x)  + g_2(A_2, x)$$.
In this step, $g_1$ and $A_1$ are kept fix.
We now learn the function $g_2$ using a gaussian process, and $A_2$ using a matrix optimization process.
We must make sure that $A_1$ and $A_2$ do not map onto the same subspace!

\item We repeat the above procedure until we have the following expression:
$$f(x) \sim g_1(A_1 x_1) + \sum_{i} g_i(A_i x_i)$$, where we refer to $g_1$ as the leading term, and $g_{i=2,...i_{max}}$ as the follow-up terms.

\end{enumerate}

The question now is: Does this structure implicitly find additive substructures between variables, and how is this better than actively finding substructures?

\subsection{What is it better than actively finding substructures}.
Actively finding substructures does find relations between different variables.
But it does not account for the weight each variable has within the subsubstruce, which then needs to be accounted by.

We will refer to a smaller function that does not influence the real function $f$ by $O(g_n)$.

We are now interested if it covers some edge cases.

We separate the algorithm into two parts:

1.) Is the model capable of learning this structure
2.) How could training result in this structure, and are the chances big enough?

--> Possible extension to the Projection pursuit regression?

\subsubsection{How does our algorithm address the shortcomings from chapter 3?}

\begin{enumerate}
\item Our algorithm intrinsically uses multiple restarts, firstly proposed as an extension to REMBO.
This makes training more reliable.
\item Our algorithm allows to not only optimize on a given domain, but also identify the subspace on which the maximal embedding is allocated on.
\item Our algorithms uses a "burn-in-rate" for the first few samples, which allows for efficient point search at the beginning, and later on switches to finding the actual, real subspace.
\item Our algorithm has faster convergence onto the real subspace, as we use the most promising models during optimization, instead of going through all possible restarts. (Approximate enhancement of loss by taking sum of last 10 losses - extrapolate, and compare it to the best example so far!)
\item Our algorithm is more accurate, as we don't assume that there is a singular maximal subspace. 
We also take into consideration that there might be perturbation on lower dimensions!
\end{enumerate}


