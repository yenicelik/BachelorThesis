\contentsline {figure}{\numberline {1}{\ignorespaces This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional function\IeC {\textquoteright }s optimizer. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space\relax }}{iv}{figure.caption.1}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Parabola Original\relax }}{10}{figure.caption.6}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Source \citep {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathbf {X}$, while the thicker red line illustrates the 1D constrained space $\mathbf {Y}$. Note that if $A \times y$ is outside of $\mathbf {X}$, it is projected onto $\mathbf {X}$ using a convex projection. The set $\mathbf {Y}$ must be chosen large enough so that the projection of its image, $A \times y $ with $y \in \mathbf {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box. \relax }}{10}{figure.caption.6}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Parabola Original\relax }}{11}{figure.caption.7}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Source \citep {Wang2013}: This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional function\IeC {\textquoteright }s optimized value $x^*$. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }}{11}{figure.caption.7}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Parabola Regret Curves on 1. UCB on vanilla Parabola, 2. \relax }}{32}{figure.caption.26}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Sinusoidal Regret Curves\relax }}{33}{figure.caption.27}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Polynomial Kernel applied to vector $[x_0, x_1]$\relax }}{34}{figure.caption.28}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Corresponding weight matrix equivalent to \ref {eq:FeatureExtension} when applied on a parabola\relax }}{34}{figure.caption.28}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Real matrix\relax }}{34}{figure.caption.29}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Matrix found by optimization algorithm\relax }}{34}{figure.caption.29}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Top-Left: The 1D Parabola which is embedded in a 2D space.\relax }}{36}{figure.caption.30}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Top-Left: The 2D Sinusoidal-Exponential Function which is embedded in a 5D space.\relax }}{37}{figure.caption.31}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Top-Left: The 2D Camelback Function which is embedded in a 5D space.\relax }}{38}{figure.caption.32}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Top-Left: The 2D Camelback Function which is embedded in a 5D space.\relax }}{39}{figure.caption.33}
\addvspace {10\p@ }
\addvspace {10\p@ }
