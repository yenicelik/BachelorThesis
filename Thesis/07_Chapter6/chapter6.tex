%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Third Chapter **********************************
%*******************************************************************************
\chapter{Evaluation}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{07_Chapter6/Figs/Raster/}{07_Chapter6/Figs/PDF/}{07_Chapter6/Figs/}}
\else
    \graphicspath{{07_Chapter6/Figs/Vector/}{07_Chapter6/Figs/}}
\fi

\section{Evaluation Settings}

Appendix A presents a list of synthetic functions and real datasets that are used to evaluate the effectiveness of a Bayesian Optimization algorithm. 
I conduct experiments in the following settings as mentioned in chapter \ref{syntheticFunction}.

\section{Quantitative evaluation}
To recapitulate, I will use log-likelihood, angle-difference measures and cumulative regret to compare the performance of different algorithms.
We present how the different algorithms operate on the UCB.
It is important to point out that all experiments capped the matrix identification step to about 30 minutes.
This is much less than in the original papers that we base the algorithm on.
The reason for this is that we want to have an acceptable comparison for medium-sized experiments, where time and computational resources can be restrictive (like on a users laptop). \\

I want to have an indication of whether the contribution of the performance comes from our subspace identification, or from the algorithm. 
For this, I start the discussion of every function in perspective starts with a graph that shows how the respective algorithm performs when the real subspace matrix $W_{\text{true}}$ is injected (instead of running tripathy's subspace identification algorithm). \\

To keep the measurements fair across algorithms, I fix the noise variance of the GP, and the kernel hyperparameters at a fixed value (for each function). \\

I run 4 different independent runs to show the variance between multiple runs.
However, as most of the runs show similar results, I display only one of them
The reader should notice that the individual runs do carry the same kernel parameters, and thus should theoretically have similar properties as UCB on the vanilla function (once the active subspace is identified).


\subsection{Parabola}

The function to be learned and optimized over is the following:

\def\WParaboa2D{
\begin{bmatrix}
    0.500\\
    0.192
\end{bmatrix}}


\begin{equation}
f(x) = \left( \WParaboa2D^T x \right)^2
\end{equation}
where we have $x \in \mathbf{R}^2$ and $W \in \mathbf{R}^{2 \times 1}$.


\paragraph{Assume $\hat{W} = W_{\text{true}}$}: I present how the respective algorithms perform if we assume that Tripathy's Stiefel Manifold optimization finds the perfect matrix.
This measures how the algorithm (and not the subspace identification) performs.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.5\textwidth]{reference_W_true/Parabola2D}
  \caption{UCB on a Parabola embedded in 2D space, when we assume that the real projection matrix is found by tripathy's algorithm.}
\end{figure}

\paragraph{Assume $\hat{W} \neq W_{\text{true}}$}: I now proceed with how different algorithms perform on the embedded parabola function which is of the following form.
This measures how the algorithm performs when the subtask of identifying the subspace has to be achieved.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.5\textwidth]{W_hat_tripathy/Parabola2D_0}
  \caption{UCB on a Parabola embedded in 2D space.
  This is when we apply tripathy's algorithm to find the real projection matrix $W$.}
\end{figure}

I ran multiple experiments, all of which performed similar to the above graph.
One can easily see that the performance on UCB using tripathy's matrix identification algorithm is similar to the case, when we assume that tripathy executes perfectly.


%One can see that REMBO without interleavings has very high variance amongst runs. 
%Sometimes it is able to find a suitable subspace (subfigure (c) and (d)), but on other runs, it fails to find an appropriate subspace.
%The reader should notice that most of these lines are linear.
%This may be the result of not tuning the hyperparameters during runs.
%In this case, the BORING algorithm assesses the exact same properties as the tripathy model (as the number of passive dimensions is set to 0), with which the two models perform very similar on the regret curves. \\

\textbf{The log likelihood of the GP w.r.t the collected datapoints} of the tripathy GP with the real matrix is comparable to the log-likelihood of the GP of the tripathy model, where the active projection matrix is calculated using the algorithm.
One should notice, however, that the angle between the found matrix and the real projection matrix is almost always at $45Â°$ - a value that does not sound very intuitive, and for which the only reasonable explanation is that the optimization problem stays the same at this projection angle.



\subsection{Camelback embedded in 3D}

\paragraph{Assume $\hat{W} = W_{\text{true}}$}: I present how the respective algorithms perform if we assume that Tripathy's Stiefel Manifold optimization finds the perfect matrix.
This measures how the algorithm (and not the subspace identification) performs.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.5\textwidth]{reference_W_true/Camelback3D}
  \caption{A picture of the same gull looking the other way!}
\end{figure}

\paragraph{Assume $\hat{W} \neq W_{\text{true}}$}: I now proceed with how different algorithms perform on the embedded parabola function which is of the following form.
This measures how the algorithm performs when the subtask of identifying the subspace has to be achieved.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[width=\textwidth]{W_hat_tripathy/Camelback3D_0}
        \label{fig:gull}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[width=\textwidth]{W_hat_tripathy/Camelback3D_1}
        \label{fig:tiger}
    \end{subfigure}   
        \caption{UCB on a Parabola embedded in 2D space. 
        This is when we apply tripathy's algorithm to find the real projection matrix $W$.}
\end{figure}

One can see that the subspace projection of rembo from 3D to 2D is not efficienty. 
This is an indication that the subspace projection is not close to the real subspace, but potentially just finds a subspace which is acceptable when higher dimensions are taken into consideration.
The difference between BORING and Tripathy is marginal.

\subsection{Camelback embedded in 5D}

\paragraph{Assume $\hat{W} = W_{\text{true}}$}: I present how the respective algorithms perform if we assume that Tripathy's Stiefel Manifold optimization finds the perfect matrix.
This measures how the algorithm (and not the subspace identification) performs.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.5\textwidth]{reference_W_true/Camelback5D}
  \caption{A picture of the same gull looking the other way!}
\end{figure}

\paragraph{Assume $\hat{W} \neq W_{\text{true}}$}: I now proceed with how different algorithms perform on the embedded parabola function which is of the following form.
This measures how the algorithm performs when the subtask of identifying the subspace has to be achieved.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[width=\textwidth]{W_hat_tripathy/Camelback5D_0}
        \label{fig:gull}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[width=\textwidth]{W_hat_tripathy/Camelback5D_1}
        \label{fig:tiger}
    \end{subfigure}   
           \caption{UCB on a Parabola embedded in 2D space.
  This is when we apply tripathy's algorithm to find the real projection matrix $W$.}
\end{figure}

One can see for for higher dimensions, tripathy's method performs well, as it is able to reduce the dimensionality of the optimization problem.
The difference between BORING and Tripathy is marginal.


\subsection{Log-Likelihood and Angle difference measures}

An interesting quantity to take into consideration is the log-likelihood of the sampled data with respect to the GP, and the angle between the found projection matrix, and the real projection matrix.
In the following, I describe the evolution of these quantities as tripathy's algorithm runs.

\paragraph{Parabola}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[width=\textwidth]{W_hat_tripathy/Camelback5D_0}
        \label{fig:gull}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[width=\textwidth]{W_hat_tripathy/Camelback5D_1}
        \label{fig:tiger}
    \end{subfigure}   
           \caption{UCB on a Parabola embedded in 2D space.
  This is when we apply tripathy's algorithm to find the real projection matrix $W$.}
\end{figure}

\paragraph{Camelback}


\paragraph{Sinusoidal}


\section{REMBO}

\begin{enumerate}
\item Talk about high variance in results
\item Talk about parameters are very sensitive
\item Talk about choosing the optimization domain
\item Talk about having no measure of knowing if the embedding is good or not.
\end{enumerate}




\section{Qualitative evaluation}
It is interesting to see, that amongst the 1000 restarts that I generated, some of the resulting matrices are close to the real projection matrix up to an absolute value of 0.01.
However, because the algorithm decides to choose the matrix with the highest likelihood, tripathy's algorithm in general does not select these matrices, but chooses a matrix that is not amongst the matrices that are very similar to the real matrices.

\subsection{Feature selection}
The goal of this task is to see, if the revised active subspace identification algorithms can effectively do feature selection.
For this task, I set up a function $ f $ that looks as follows:

\def\B{
\begin{bmatrix}
    (x - a_0)^2 \\
    (y - a_1)^2
\end{bmatrix}}

\begin{equation} \label{eq:FeatureExtension}
f \left( W \B \right) \approx g \left( x \right)
\end{equation} 

where $x_0, x_1$ are constants. \\

For this specific experiment, the function $f$ is chosen to be a one-dimensional parabola. 
As such, $W$ is chosen as a matrix on the Stiefel manifold with dimensions $\mathbf{R}^{d \times D}$

Doing a feature extension over $x$ and $y$, we can get the following feature representation:

\def\PHI{
\begin{bmatrix}
	x_0^2 \\
	x_1^2 \\
	x_0 \\
	x_1 \\
    1
\end{bmatrix}}


\def\WtoPhi{
\begin{bmatrix}
	w_0 \\
    w_1 \\
	-2 w_0 a_0 \\
	-2 w_1 a_1 \\
	w_0 a_0^2 + w_1 a_1^2
\end{bmatrix}}

\begin{figure}[h]

\begin {minipage}{0.47\textwidth}
  \centering
  \begin{equation}
    \PHI
  \end{equation}
  \caption{Polynomial Kernel applied to vector $[x_0, x_1]$}
\end{minipage}
\hfill
\begin {minipage}{0.47\textwidth}
  \centering
  \begin{equation}
    \WtoPhi
  \end{equation}
  \caption{Corresponding weight matrix equivalent to \ref{eq:FeatureExtension} when applied on a parabola}
\end{minipage}

\end{figure}

To run experiments, I instantiate the "real" matrix, which should be found by the algorithm with the values $w_0 = 0.589$, $w_1 = 0.808$ (randomly sampled as a matrix on the Stiefel manifold), $a_0 = -0.1$, $a_1 = 0.1$ (chosen by me as coefficients). \\

I apply the algorithm 1. from \citep{Tripathy} to identify the active projection matrix.
The optimization algorithm has 50 samples to discover the hidden matrix, which it seemingly does up do a certain degree of accuracy.
Similar results are achieved for repeated tries.
The following figure shows the real matrix, and the matrix the algorithm has found.

\def\realW{
\begin{bmatrix}
	0.589 \\
    0.808 \\
	0.118 \\
	-0.162 \\
	0.823
\end{bmatrix}}

\def\okW1{
\begin{bmatrix}
	-0.355 \\
    	-0.533 \\
    	-0.908 \\
    	0.099 \\
    -0.756 
\end{bmatrix}}

\begin{figure}[h] 
\begin {minipage}{0.47\textwidth}
  \centering
  \begin{equation} \label{fig:realMatrix}
    \realW
  \end{equation}
  \caption{Real matrix}
\end{minipage}
\hfill
\begin {minipage}{0.47\textwidth}
  \centering
  \begin{equation} \label{fig:foundMatrix}
    \okW1
  \end{equation}
  \caption{Matrix found by optimization algorithm}
\end{minipage}
\end{figure}

Although one can see that the element wise difference between the two matrices \ref{fig:realMatrix} and \ref{fig:foundMatrix} are high (between $0.05$ and $0.15$), one can see that the matrix recovery is successful in finding an approximate structure that resembles the original structure of the features.
One should observe that the found matrix is an approximate solution to the real matrix in the projection. I.e. the matrix found is close to the real matrix, but multiplied by $-1$. \\

Because in this case, I applied the feature selection algorithm on a vector-matrix (only one column), one can quantify the reconstruction of the real matrix through the found matrix by the normalized scalar product.
This quantity is a metric between $0$ and $1$, where $0$ means that both vectors are orthogonal, and $1$ means that both vectors overlap.

\begin{equation}
\text{overlap}(u, v) = \frac{| \langle u, v \rangle |}{\langle u, u \rangle}
\end{equation}
where $u$ is the real vector, and $v$ is the found vector.

Inserting the actual values into the field, we get $0.79$, which is a good value for the feature vector found, and the trained number of datapoints which is 50. \\
 
 This experiment shows that algorithm 1. from \citep{Tripathy} successfully allows a viable option to other feature selection algorithms, by providing a measure, where the optimal linear projection is found. 
 However, one must notice that other feature selection algorithms (such as SVM), are more efficient, and will provide better results with higher probability if applied on a similar kernel. \\
 
 One major observation I made was the the increase in the log likelihood of the data w.r.t. the projection matrix did not correlate with the decrease in the angle between the real vs the found projection matrix.
 Also, most often, the angle was at around 40 degrees, which means that only slight improvements over a fully random embedding were made.


\subsection{Subspace identification}
One of the main reasons to use our method is because we allow for subspace identification.
We have the following functions:

\begin{enumerate}
\item 1D Parabola embedded in a 2D space
\item 2D Camelback embedded in a 5D space
\item 2D Sinusoidal and Exponential function embedded in a 5D space
\end{enumerate}

To be able to visualize the points, I proceed with the following procedure:

I generate testing points (points to be visualized) within the 2D-space in a uniform grid.
I then project these testing points to the dimension of the original function ($2d$ for parabola, else $5d$).
I then let each algorithm learn and predict the projection matrix and GP mean predictions.
If because the transformation from $2D$ space, to $5D$ space and to GP mean prediction is each bijective, we can visualize the $2D$ points with the GP mean prediction right away.
As such, the dimension of the embedding learned does not have impact on the visualization!

In the following figures, blue point shows the sampled real function value.
Orange points shows the sampled mean prediction of the trained GP.
The GPs were each trained on 100 datapoints. 
The points shown below were not used for training at any point, as these are included in the test-set.

% PARABOLA FUNCTION
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.30\textwidth}
        \includegraphics[width=\textwidth]{orig/Parabola-2D-_1D.png}
        \caption{Parabola Original}
        \label{fig:gull}
    \end{subfigure}
    %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
        \includegraphics[width=\textwidth]{orig/Parabola-2D-_1D_BORING.png}
        \caption{Parabolanusoidal Boring}
        \label{fig:tiger}
    \end{subfigure}
     %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \vskip\baselineskip
    \begin{subfigure}[b]{0.30\textwidth}
        \includegraphics[width=\textwidth]{orig/Parabola-2D-_1D_TRIPATHY.png}
        \caption{Parabola Tripathy}
        \label{fig:mouse}
    \end{subfigure}
            %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.30\textwidth}
        \includegraphics[width=\textwidth]{orig/Parabola-2D-_1D_REMBO.png}
        \caption{Parabola Rembo}
        \label{fig:mouse}
    \end{subfigure}
    \caption{Top-Left: The 1D Parabola which is embedded in a 2D space.}\label{fig:animals}
\end{figure}

I set the number of restarts to $14$ and number of randomly sampled datapoints to 100.
Notice that the Tripathy approximation is slightly more accurate than the BORING approximation. 
This is because one of Tripathy's initial starting points were selected better, such that algorithm 3 ran many times before the relative loss terminated the algorithm.
The active subspace projection matrix is of size $\mathbf{R}^{1 \times 2}$

% SINUSOIDAL FUNCTION
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Sinusoidal-5D-_2D.png}
        \caption{Sinusoidal Original}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Sinusoidal-5D-_2D_BORING.png}
        \caption{Sinusoidal Boring}
        \label{fig:tiger}
    \end{subfigure}
        \vskip\baselineskip
 %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Sinusoidal-5D-_2D_TRIPATHY.png}
        \caption{Sinusoidal Tripathy}
        \label{fig:mouse}
    \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Sinusoidal-5D-_2D_REMBO.png}
        \caption{Sinusoidal Rembo}
        \label{fig:mouse}
    \end{subfigure}
    \caption{Top-Left: The 2D Sinusoidal-Exponential Function which is embedded in a 5D space.}\label{fig:animals}
\end{figure}

I set the number of restarts to $28$ and number of randomly sampled datapoints to 100.
The active subspace projection matrix is of size $\mathbf{R}^{1\times 5}$, as this is a function that exhibits a strong principal component, but that still attains small perturbations among a different dimension.
One can see very well here, that BORING is able to take into account the small perturbations, at a considerably lower cost than Tripathy would be able to.


% CAMELBACK FUNCTION
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Camelback-5D-_2D.png}
        \caption{Camelback Original}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Camelback-5D-_2D_BORING.png}
        \caption{Camelback Boring}
        \label{fig:tiger}
    \end{subfigure}
        \vskip\baselineskip
 %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Camelback-5D-_2D_TRIPATHY.png}
        \caption{Camelback Tripathy}
        \label{fig:mouse}
    \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{orig/Camelback-5D-_2D_REMBO.png}
        \caption{Camelback Rembo}
        \label{fig:mouse}
    \end{subfigure}
    \caption{Top-Left: The 2D Camelback Function which is embedded in a 5D space.}\label{fig:animals}
\end{figure}

I set the number of restarts to $28$ and number of randomly sampled datapoints to 100.
The active subspace projection matrix is of size $\mathbf{R}^{2 \times 5}$, as this is a function that lives in a 2D space, and has two strong principal components.
Notice that Tripathy and BORING use the exact same algorithm, as the visualization does not allow to add a third axis. 
In other words, BORING does not add any additional orthogonal vector to the model.
As such, it does not add any additional kernels to the model aswell, and is equivalent to tripathy.
