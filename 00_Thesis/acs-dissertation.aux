\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Project Proposal for Bachelor Thesis}{vii}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.0.1}Motivation}{vii}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.0.2}Background}{viii}}
\citation{Tripathy}
\citation{Rolland}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.0.3}Scope of the Project}{ix}}
\citation{Wang}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  This function in D=2 dimesions only has d=1 effective dimension. Hence, the 1-dimensional embedding includes the 2-dimensional function’s optimizer. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space\relax }}{x}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Bayesian Optimization in high dimensions}{xi}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Gaussian Processes}{xii}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Derivation of the Gaussian Process Formula}{xiii}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Acquisition Functions}{xv}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Upper Confident Bound (UCB)}{xv}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Probability of Improvement (PI)}{xv}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Expected Improvement (EI)}{xvi}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Resources}{xvii}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{xix}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch3}{{3}{xix}}
\newlabel{ch3@cref}{{[chapter][3][]3}{xix}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Projection matrix based algorithms}{xx}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Active learning of linear subspace}{xx}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Simultaneous active learning of functions and their linear embeddings (pseudocode) :: Active learning of linear subspace {Garnett2013}\relax }}{xx}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}High dimensional Gaussian bandits}{xxi}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces The SI-BO algorithm {Djolonga2013}\relax }}{xxi}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Random embeddings (REMBO)}{xxii}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Parabola Original\relax }}{xxiii}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gull}{{3.1}{xxiii}}
\newlabel{fig:gull@cref}{{[figure][1][3]3.1}{xxiii}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Taken from {Wang2013}: Embedding from $d = 1$ into $D=2$. The box illustrates the 2D constrained space $\mathsf  {X}$, while the thicker red line illustrates the 1D constrained space $\mathsf  {Y}$. Note that if $Ay$ is outside $\mathsf  {X}$, it is projected onto $\mathsf  {X}$. The set $\mathsf  {Y}$ must be chosen large enough so that the projection of its image, $A\mathsf  {Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box). \relax }}{xxiii}}
\newlabel{fig:animals}{{3.2}{xxiii}}
\newlabel{fig:animals@cref}{{[figure][2][3]3.2}{xxiii}}
\citation{Wang}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Parabola Original\relax }}{xxv}}
\newlabel{fig:gull}{{3.3}{xxv}}
\newlabel{fig:gull@cref}{{[figure][3][3]3.3}{xxv}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Taken from {Wang2013} Fig. 2: This function in D=2 dimesions only has d=1 effective dimension: the vertical axis indicated with the word important on the right hand side figure. Hence, the 1-dimensional embedding includes the 2-dimensional function’s optimizer. It is more efficient to search for the optimum along the 1-dimensional random embedding than in the original 2-dimensional space. \relax }}{xxv}}
\newlabel{fig:animals}{{3.4}{xxv}}
\newlabel{fig:animals@cref}{{[figure][4][3]3.4}{xxv}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Applications to high-dimensional uncertainty propogation}{xxvi}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.4.0.1}I now proceed with a more detailed description of the algorithm.}{xxvi}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.1}Kernel used}{xxvii}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.2}Step 1.: Determine the active projection matrix W}{xxviii}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.3}Step 2.: Optimizing over GP noise variance and the kernel hyperparameters}{xxx}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.4}Additional details}{xxx}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4.5}Identification of active subspace dimension }{xxxi}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Algorithms that exploit additive substructures}{xxxi}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Independent additive structures within the target function}{xxxi}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Additional approaches}{xxxii}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Elastic Gaussian Processes}{xxxii}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Bayesian Optimization using Dropout}{xxxii}}
\citation{morin05}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Model Design and Training}{xxxv}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch4}{{4}{xxxv}}
\newlabel{ch4@cref}{{[chapter][4][]4}{xxxv}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Model}{xxxv}}
\citation{mikolov13b}
\newlabel{eq:ch4:bengio_softmax}{{4.1}{xxxvi}}
\newlabel{eq:ch4:bengio_softmax@cref}{{[equation][1][4]4.1}{xxxvi}}
\newlabel{eq:ch4:skip_softmax}{{4.2}{xxxvi}}
\newlabel{eq:ch4:skip_softmax@cref}{{[equation][2][4]4.2}{xxxvi}}
\newlabel{fig:chap4:original_nnlm}{{4.1a}{xxxvii}}
\newlabel{fig:chap4:original_nnlm@cref}{{[subfigure][1][4,1]4.1a}{xxxvii}}
\newlabel{sub@fig:chap4:original_nnlm}{{a}{xxxvii}}
\newlabel{sub@fig:chap4:original_nnlm@cref}{{[subfigure][1][4,1]4.1a}{xxxvii}}
\newlabel{fig:chap4:hierarchical}{{4.1b}{xxxvii}}
\newlabel{fig:chap4:hierarchical@cref}{{[subfigure][2][4,1]4.1b}{xxxvii}}
\newlabel{sub@fig:chap4:hierarchical}{{b}{xxxvii}}
\newlabel{sub@fig:chap4:hierarchical@cref}{{[subfigure][2][4,1]4.1b}{xxxvii}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Vocabulary structures.}}{xxxvii}}
\newlabel{fig:chap4:vocab}{{4.1}{xxxvii}}
\newlabel{fig:chap4:vocab@cref}{{[figure][1][4]4.1}{xxxvii}}
\newlabel{eq:ch4:morin}{{4.3}{xxxvii}}
\newlabel{eq:ch4:morin@cref}{{[equation][3][4]4.3}{xxxvii}}
\citation{huffman52}
\citation{hinton12}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Skip-gram network architecture. }}{xxxix}}
\newlabel{fig:chap4:skipgram}{{4.2}{xxxix}}
\newlabel{fig:chap4:skipgram@cref}{{[figure][2][4]4.2}{xxxix}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Training}{xl}}
\newlabel{eq:ch4:obj}{{4.4}{xl}}
\newlabel{eq:ch4:obj@cref}{{[equation][4][4]4.4}{xl}}
\newlabel{eq:ch4:sigma}{{4.5}{xl}}
\newlabel{eq:ch4:sigma@cref}{{[equation][5][4]4.5}{xl}}
\newlabel{eq:ch4:sol}{{4.6}{xli}}
\newlabel{eq:ch4:sol@cref}{{[equation][6][4]4.6}{xli}}
\newlabel{eq:ch4:code}{{4.7}{xli}}
\newlabel{eq:ch4:code@cref}{{[equation][7][4]4.7}{xli}}
\newlabel{eq:ch4:dmain}{{4.8}{xlii}}
\newlabel{eq:ch4:dmain@cref}{{[equation][8][4]4.8}{xlii}}
\newlabel{eq:ch4:dc}{{4.9}{xlii}}
\newlabel{eq:ch4:dc@cref}{{[equation][9][4]4.9}{xlii}}
\newlabel{eq:ch4:dh}{{4.10}{xlii}}
\newlabel{eq:ch4:dh@cref}{{[equation][10][4]4.10}{xlii}}
\newlabel{eq:ch4:grad}{{4.11}{xlii}}
\newlabel{eq:ch4:grad@cref}{{[equation][11][4]4.11}{xlii}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Outline of training process.\relax }}{xliii}}
\newlabel{alg:chap4:backprop}{{3}{xliii}}
\newlabel{alg:chap4:backprop@cref}{{[algorithm][3][]3}{xliii}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Implementation}{xliv}}
\newlabel{lst:chap4:dropout}{{4.1}{xliv}}
\newlabel{lst:chap4:dropout@cref}{{[listing][1][4]4.1}{xliv}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}Cython code for dropout regularisation at the hidden layer.}{xliv}}
\newlabel{lst:chap4:sigmoid}{{4.2}{xlv}}
\newlabel{lst:chap4:sigmoid@cref}{{[listing][2][4]4.2}{xlv}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}Cython code for precomputing and accessing sigmoid function.}{xlv}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Discretisation of the sigmoid function. }}{xlvi}}
\newlabel{fig:chap4:sigmoid}{{4.3}{xlvi}}
\newlabel{fig:chap4:sigmoid@cref}{{[figure][3][4]4.3}{xlvi}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Measuring brand perception}{xlviii}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Evaluation}{li}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch5}{{5}{li}}
\newlabel{ch5@cref}{{[chapter][5][]5}{li}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Evaluation setting}{li}}
\citation{bamman14}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Quantitative evaluation}{lii}}
\newlabel{ch5:2}{{5.2}{lii}}
\newlabel{ch5:2@cref}{{[section][2][5]5.2}{lii}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Location}{lii}}
\newlabel{fig:chap5:loc_prec}{{5.1a}{liv}}
\newlabel{fig:chap5:loc_prec@cref}{{[subfigure][1][5,1]5.1a}{liv}}
\newlabel{sub@fig:chap5:loc_prec}{{a}{liv}}
\newlabel{sub@fig:chap5:loc_prec@cref}{{[subfigure][1][5,1]5.1a}{liv}}
\newlabel{fig:chap5:loc_mrr}{{5.1b}{liv}}
\newlabel{fig:chap5:loc_mrr@cref}{{[subfigure][2][5,1]5.1b}{liv}}
\newlabel{sub@fig:chap5:loc_mrr}{{b}{liv}}
\newlabel{sub@fig:chap5:loc_mrr@cref}{{[subfigure][2][5,1]5.1b}{liv}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Location evaluation results.}}{liv}}
\newlabel{fig:chap5:loc_eval}{{5.1}{liv}}
\newlabel{fig:chap5:loc_eval@cref}{{[figure][1][5]5.1}{liv}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Gender}{lvi}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Gender evaluation dataset.\relax }}{lvii}}
\newlabel{tab:chap5:gender}{{5.1}{lvii}}
\newlabel{tab:chap5:gender@cref}{{[table][1][5]5.1}{lvii}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Gender evaluation results.\relax }}{lviii}}
\newlabel{tab:chap5:gender_eval}{{5.2}{lviii}}
\newlabel{tab:chap5:gender_eval@cref}{{[table][2][5]5.2}{lviii}}
\citation{finkelstein02}
\citation{hill14}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}General Semantic Similarity}{lix}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison of WordSim-353 and SimLex-999 evaluation datasets.\relax }}{lix}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces General semantic similarity results.\relax }}{lx}}
\newlabel{tab:chap5:sem_sim}{{5.4}{lx}}
\newlabel{tab:chap5:sem_sim@cref}{{[table][4][5]5.4}{lx}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Qualitative evaluation}{lx}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Terms with the biggest cosine similarity to \textit  {cheers} for people in US/UK\relax }}{lxi}}
\newlabel{tab:chap5:reverse_dic}{{5.5}{lxi}}
\newlabel{tab:chap5:reverse_dic@cref}{{[table][5][5]5.5}{lxi}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Terms with the biggest cosine similarity to \textit  {work} for people in each age group.\relax }}{lxii}}
\newlabel{tab:chap5:reverse_dic_age}{{5.6}{lxii}}
\newlabel{tab:chap5:reverse_dic_age@cref}{{[table][6][5]5.6}{lxii}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Brand perception analysis}{lxii}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces 2014 BrandIndex Annual Ranking for Internet Search}}{lxiii}}
\newlabel{tab:chap5:search}{{5.7}{lxiii}}
\newlabel{tab:chap5:search@cref}{{[table][7][5]5.7}{lxiii}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Correlation coefficients and p-values for each category.\relax }}{lxiii}}
\newlabel{tab:chap5:brand_eval_corr}{{5.8}{lxiii}}
\newlabel{tab:chap5:brand_eval_corr@cref}{{[table][8][5]5.8}{lxiii}}
\newlabel{RF1}{64}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Full ranking result of the CDSGD model.\relax }}{lxiv}}
\newlabel{tab:chap5:brand_eval}{{5.9}{lxiv}}
\newlabel{tab:chap5:brand_eval@cref}{{[table][9][5]5.9}{lxiv}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions and Future Work}{lxv}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch6}{{6}{lxv}}
\newlabel{ch6@cref}{{[chapter][6][]6}{lxv}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Summary}{lxv}}
\citation{mikolov13c}
\citation{vilnis14}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future work}{lxvii}}
\citation{mitchell10}
\bibstyle{unsrt}
\bibdata{dissertation}
\bibcite{makens65}{1}
\bibcite{almenberg14}{2}
\bibcite{mantonakis13}{3}
\bibcite{kuehn76}{4}
\bibcite{gupta88}{5}
\bibcite{trusov13}{6}
\bibcite{tirunillai14}{7}
\bibcite{tirunillai12}{8}
\bibcite{thomas06}{9}
\bibcite{ashok13}{10}
\bibcite{harris54}{11}
\bibcite{bengio03}{12}
\bibcite{rumelhart88}{13}
\bibcite{mikolov13}{14}
\bibcite{le14}{15}
\bibcite{vilnis14}{16}
\bibcite{yang13}{17}
\bibcite{morin05}{18}
\bibcite{mikolov13b}{19}
\bibcite{huffman52}{20}
\bibcite{hinton12}{21}
\bibcite{bamman14}{22}
\bibcite{finkelstein02}{23}
\bibcite{hill14}{24}
\bibcite{mikolov13c}{25}
\bibcite{mitchell10}{26}
